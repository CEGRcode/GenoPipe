#!/bin/bash
#PBS -l nodes=1:ppn=4
#PBS -l pmem=16gb
#PBS -l walltime=01:00:00
#PBS -A open
#PBS -o logs/mix.human.50M.eid.log.out
#PBS -e logs/mix.human.50M.eid.log.err
#PBS -t 1-1000

# Each index of the job array t:[1,16] represents a different set of mixed simulations. This script will execute EpitopeID against all all simulated datasets at various titrations and use the perl script to calculate the detection statistics. Each index corresponds to a different titration mix.

# This script will check that 2000 FASTQ files have been generated before executing EpitopeID.

WRK=/path/to/GenoPipe/paper/SyntheticEpitope
THREADS=6
EPITOPEID=$WRK/../../EpitopeID
cd $WRK

module load bedtools
module load samtools
module load anaconda3
source activate genopipe

REF=hg19
# DEPTH=20M
DEPTH=50M
EPITOPE=R500
AFACTOR=CTCF
BFACTOR=POLR2H

DATABASE=$WRK/../db/$REF\_EpiID
[ -d logs ] || mkdir logs

# Titrate out various ratios
for PER in 90 80 70 60 50 40 30 20 10
do
	# Construct results directory pathname
	RESULTS=$WRK/results/mix_human/$DEPTH/$PER\_$AFACTOR\_$BFACTOR
	[ -d $RESULTS/ID ] || mkdir $RESULTS/ID
	[ -d $RESULTS/runtime ] || mkdir $RESULTS/runtime
	echo $RESULT

	# Set-up Temp directory
	TEMP=$WRK/temp-$PER-$PBS_ARRAYID-$DEPTH
	[ -d $TEMP ] || mkdir $TEMP

	cd $TEMP
	FQ=$RESULTS/FASTQ/Simulation_$PBS_ARRAYID
	[ -f Simulation_$PBS_ARRAYID\_R1.fastq.gz ] || ln -s $FQ\_R1.fastq.gz
	[ -f Simulation_$PBS_ARRAYID\_R2.fastq.gz ] || ln -s $FQ\_R2.fastq.gz

	# Execute EpitopeID and record time
	cd $EPITOPEID
	start=`date +%s`
	bash identify-Epitope.sh -i $TEMP -o $RESULTS/ID -d $DATABASE -t $THREADS
	end=`date +%s`
	runtime=$((end-start))
	echo "Experiment $EXPERIMENT.$DEPTH.$PBS_ARRAYID ($RESULTS) finished in ${runtime}" > $RESULTS/runtime/Simulation_$PBS_ARRAYID.runtime

	# Clean-up
	rm -r $TEMP
done
